{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1bdzGsAZBGwPDhvGutGmklDGm9pblHXK-",
      "authorship_tag": "ABX9TyOR6Ej1rvSpvfAUwpEN5/OQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damola936/AI-ML-LLM/blob/main/PersonalWorker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H9civO937l1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy11_5nw6mUk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1E_x93bG70UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CbL3XRts8HDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "from langchain.document_loaders import DirectoryLoader, TextLoader, UnstructuredWordDocumentLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI, ChatOllama\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "_R-bArAE69UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# price is a factor for our company, so we're going to use a low cost model\n",
        "\n",
        "MODEL = \"gpt-4o-mini\"\n",
        "db_name = \"vector_db\""
      ],
      "metadata": {
        "id": "tV9fdF-08ocd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "WC74NnMH8pyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RJv6xOZXCoNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx docx"
      ],
      "metadata": {
        "collapsed": true,
        "id": "RzTICWyBCzFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get list of directories or individual files\n",
        "base_path = \"/content/drive/MyDrive\"\n",
        "\n",
        "# Optional: only get directories or specific folders if needed\n",
        "documents = []\n",
        "text_loader_kwargs = {'encoding': 'utf-8'}\n",
        "\n",
        "# Step 2: Use DirectoryLoader only on folders, not individual files\n",
        "loader = DirectoryLoader(\n",
        "    path=base_path,  # not a list, a single folder path\n",
        "    glob=\"**/*.docx\",\n",
        "    loader_cls=UnstructuredWordDocumentLoader,\n",
        "    loader_kwargs=text_loader_kwargs\n",
        ")\n",
        "\n",
        "# Step 3: Load all matching files recursively\n",
        "documents = loader.load()\n",
        "\n",
        "# Step 4: Split into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(documents)\n",
        "print(f\"Total number of chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "id": "7-UiKNGN-KVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating embeddings"
      ],
      "metadata": {
        "id": "ymOj3d7jDIP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tnCkpuT5DJxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TBZ-2t2QDxM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vectorstore\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
        "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
      ],
      "metadata": {
        "id": "N_TVTmQwDOPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's investigate the vectors\n",
        "\n",
        "collection = vectorstore._collection\n",
        "count = collection.count()\n",
        "\n",
        "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
        "dimensions = len(sample_embedding)\n",
        "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
      ],
      "metadata": {
        "id": "lmPKKaewD-GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
        "vectors = np.array(result['embeddings'])\n",
        "documents = result['documents']\n",
        "metadatas = result['metadatas']"
      ],
      "metadata": {
        "id": "GHFdna1qEH3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We humans find it easier to visalize things in 2D!\n",
        "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
        "# (t-distributed stochastic neighbor embedding)\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=20)\n",
        "reduced_vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "# Create the 2D scatter plot\n",
        "fig = go.Figure(data=[go.Scatter(\n",
        "    x=reduced_vectors[:, 0],\n",
        "    y=reduced_vectors[:, 1],\n",
        "    mode='markers',\n",
        "    marker=dict(size=5, opacity=0.8),\n",
        "    text=[f\"<br>Text: {d[:100]}...\" for d in documents],\n",
        "    hoverinfo='text'\n",
        ")])\n",
        "\n",
        "fig.update_layout(\n",
        "    title='2D Chroma Vector Store Visualization',\n",
        "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
        "    width=800,\n",
        "    height=600,\n",
        "    margin=dict(r=20, b=10, l=10, t=40)\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "bxXA3R_nEmBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsne = TSNE(n_components=3, random_state=42, perplexity=20)\n",
        "reduced_vectors = tsne.fit_transform(vectors)\n",
        "\n",
        "# Create the 3D scatter plot\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=reduced_vectors[:, 0],\n",
        "    y=reduced_vectors[:, 1],\n",
        "    z=reduced_vectors[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(size=5, opacity=0.8),\n",
        "    text=[f\"<br>Text: {d[:100]}...\" for d in documents],\n",
        "    hoverinfo='text'\n",
        ")])\n",
        "\n",
        "fig.update_layout(\n",
        "    title='3D Chroma Vector Store Visualization',\n",
        "    scene=dict(xaxis_title='x',yaxis_title='y', zaxis_title='z'),\n",
        "    width=900,\n",
        "    height=700,\n",
        "    margin=dict(r=20, b=10, l=10, t=40)\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "hYs3MB7fGBQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a UI"
      ],
      "metadata": {
        "id": "91FVIwo5Gy-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.7, model=MODEL, openai_api_key=OPENAI_API_KEY) # model\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) # model memory\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 30}) # vectorstore as retriever\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory) # creating conversion chain"
      ],
      "metadata": {
        "id": "1cTI2q9OHBGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(question, history):\n",
        "    result = conversation_chain.invoke({\"question\":question})\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "HOdEuXQsG1Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=False)"
      ],
      "metadata": {
        "id": "MJhrTvnhIAQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Opensource Models"
      ],
      "metadata": {
        "id": "nSROj5HoqvUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch accelerate bitsandbytes transformers"
      ],
      "metadata": {
        "id": "ggI2TV7NzrTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "l7ehc9Pl1zPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_id=\"meta-llama/Llama-3.1-8B\"\n",
        "\n",
        "# Load in 8-bit precision to reduce memory usage\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    load_in_8bit=True,  # Quantization to save memory\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Create pipeline with more controlled parameters\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,  # Limit output size\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")"
      ],
      "metadata": {
        "id": "H9y93U_dqxHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap in LangChain's HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline) # opensource model\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) # model memory\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 30}) # vectorstore as retriever\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory) # creating conversion chain"
      ],
      "metadata": {
        "id": "D8E7lPRs4Zd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(question, history):\n",
        "    result = conversation_chain.invoke({\"question\":question})\n",
        "    return result[\"answer\"]"
      ],
      "metadata": {
        "id": "U-NeldaErDmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=False) # Opensource model not good enough as 8B parameters"
      ],
      "metadata": {
        "id": "NeF6o8E3rESQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}