{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damola936/AI-ML-LLM/blob/main/AbstractOptimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8xcrcoZycPm"
      },
      "source": [
        "# Skim Literature 🤖📜\n",
        "\n",
        "The purpose of this Notebook is to build a model that helps making reading medical papers easier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv3POp577Z9k"
      },
      "source": [
        "## Checking for GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y79SmglB7bza"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpKqkSY_7eN_"
      },
      "source": [
        "## Getting Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWHwqRlX7jH9"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/refs/heads/main/extras/helper_functions.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2FR-CgA7nY6"
      },
      "outputs": [],
      "source": [
        "from helper_functions import plot_loss_curves, unzip_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDmVr6l07q6L"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "# Create a tensorboard callback\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "    \"\"\"\n",
        "        Creates a tensorboard callback to use when training\n",
        "    \"\"\"\n",
        "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y-%m-%d <-> %H-%M-%S\")\n",
        "    file_writer = tf.summary.create_file_writer(log_dir) # Create a FileWriter for TensorBoard\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "    print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "    return tensorboard_callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjs-cbDc71YY"
      },
      "source": [
        "## Getting a Dataset (PubMed 200K RCT Dataset)\n",
        "\n",
        "https://github.com/Franck-Dernoncourt/pubmed-rct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wpn8idO77lK"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
        "!ls pubmed-rct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA9cjzm29GSx"
      },
      "source": [
        "## Check What files are in the `PubMed20K` dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCMXX0cI9K9I"
      },
      "outputs": [],
      "source": [
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_0sVKrJ_PHc"
      },
      "source": [
        "## Exploring Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6nH03n8_RRb"
      },
      "outputs": [],
      "source": [
        "# Creating function to read line of our document\n",
        "def read_line(document):\n",
        "    \"\"\"\n",
        "        Reads a document and returns the lines of text as a list\n",
        "    \"\"\"\n",
        "    with open(document, mode=\"r\") as file:\n",
        "        data = file.readlines()\n",
        "        print(data[:10])\n",
        "\n",
        "document = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt\"\n",
        "read_line(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpZTckr2DMl4"
      },
      "source": [
        "How do we want our data to look...\n",
        "```\n",
        "[{\n",
        "    \"line_number\": 0,\n",
        "\n",
        "    \"target\": \"OBJECTIVE\",\n",
        "\n",
        "    \"text\" : \"To investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , ...\n",
        "    \n",
        "    \"total_lines\" : 11\n",
        "}] ...\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PxA3gAPFJi2"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_document(document):\n",
        "    \"\"\"\n",
        "        Preprocesses document into a structured dictionary\n",
        "        for model training\n",
        "    \"\"\"\n",
        "    final_list = []\n",
        "    pattern = r'^###.*$' # We want to filter based on lines with text that begin with \"###\"\n",
        "\n",
        "    with open(document, mode=\"r\") as file:\n",
        "        lines = file.readlines()\n",
        "        text = \"\".join(lines)\n",
        "        # Split text based on the pattern\n",
        "        split_text = re.split(pattern, text, flags=re.MULTILINE)\n",
        "        # Remove empty strings based on result\n",
        "        sections = [[section.strip()] for section in split_text if section.strip()]\n",
        "        # Iterate over our sections and create our target dictionary format\n",
        "        for section in sections:\n",
        "            section_text_list = section[0].split(\"\\n\")\n",
        "            for index, text in enumerate(section_text_list):\n",
        "                data_dict = {\n",
        "                    \"line_number\": index,\n",
        "                    \"target\": text.split(\"\\t\")[0],\n",
        "                    \"text\": text.split(\"\\t\")[1],\n",
        "                    \"total_lines\": len(section_text_list) - 1\n",
        "                }\n",
        "                final_list.append(data_dict)\n",
        "\n",
        "        return final_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IobENxIFWds_"
      },
      "outputs": [],
      "source": [
        "data = preprocess_document(document)[12:24]\n",
        "for d in data:\n",
        "    print(d)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZw5X-6wRolc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "train_document = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt\"\n",
        "test_document = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt\"\n",
        "val_document = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt\"\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "train_data = preprocess_document(train_document)\n",
        "test_data = preprocess_document(test_document)\n",
        "val_data = preprocess_document(val_document)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(len(train_data), len(test_data), len(val_data))\n",
        "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpbZr8dTZlzY"
      },
      "source": [
        "Turn our data into a dataframe to better visualize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "056vz7GwZsPT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.DataFrame(train_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "val_df = pd.DataFrame(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZUc7nOzZ9xM"
      },
      "outputs": [],
      "source": [
        "train_df.head(13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OFCXvsjaAMm"
      },
      "outputs": [],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FduyU0CaBl5"
      },
      "outputs": [],
      "source": [
        "val_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gr-YzLrFa6wC"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def check_distribution(column, is_rotate=True):\n",
        "    # Viewing Visually\n",
        "    ax = sns.histplot(data=train_df, x=column, bins=10)\n",
        "    plt.title(f\"Distribution of {column} variables\")\n",
        "\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fontsize=10)\n",
        "\n",
        "    plt.gca().spines[[\"top\", \"right\"]].set_visible(False)\n",
        "    if is_rotate:\n",
        "        plt.xticks(rotation=70)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e35DibmOcXHp"
      },
      "outputs": [],
      "source": [
        "check_distribution(\"target\", is_rotate=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vXN7Pk6cY9_"
      },
      "outputs": [],
      "source": [
        "check_distribution(\"total_lines\", is_rotate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYfZUa4DdFi4"
      },
      "source": [
        "## Get list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2tq8rmsdIW1"
      },
      "outputs": [],
      "source": [
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "print(len(train_sentences), len(val_sentences), len(test_sentences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mRpmskRdp9h"
      },
      "outputs": [],
      "source": [
        "# View first 10 lines of training sentences\n",
        "train_sentences[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK_CauG3ezMq"
      },
      "source": [
        "## Make Numeric Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMas9fU0e1Pb"
      },
      "outputs": [],
      "source": [
        "# One Hot encode Labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(\n",
        "    train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "\n",
        "val_labels_one_hot = one_hot_encoder.transform(\n",
        "    val_df[\"target\"].to_numpy().reshape(-1, 1)\n",
        ")\n",
        "\n",
        "test_labels_one_hot = one_hot_encoder.transform(\n",
        "    test_df[\"target\"].to_numpy().reshape(-1, 1)\n",
        ")\n",
        "# Checking labels...\n",
        "train_labels_one_hot, val_labels_one_hot, test_labels_one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1vDrGs_hcJU"
      },
      "source": [
        "## Label Encode Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHf6mfoVhfWR"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_le = label_encoder.fit_transform(\n",
        "    train_df[\"target\"].to_numpy()\n",
        ")\n",
        "val_labels_le = label_encoder.transform(\n",
        "    val_df[\"target\"].to_numpy()\n",
        ")\n",
        "test_labels_le = label_encoder.transform(\n",
        "    test_df[\"target\"].to_numpy()\n",
        ")\n",
        "\n",
        "train_labels_le, val_labels_le, test_labels_le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31kYutfHis1G"
      },
      "outputs": [],
      "source": [
        "# Get class names and number of classes from LabeEncoder Instance\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys3JKtU8jyI_"
      },
      "source": [
        "# Creating our Baseline Model (Model 0: A NaiveBayes Classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkYVtPKvk6wS"
      },
      "outputs": [],
      "source": [
        "models_metrics = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7FueWIBnkIs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "def add_model_evaluation_to_list(name, val_preds):\n",
        "    model_metrics = {\n",
        "    \"name\": name,\n",
        "\n",
        "    \"accuracy\": accuracy_score(\n",
        "        val_labels_le, val_preds\n",
        "        ),\n",
        "\n",
        "    \"f1\" : f1_score(\n",
        "        val_labels_le, val_preds, average=\"weighted\"\n",
        "    ),\n",
        "\n",
        "    \"precision\" : precision_score(\n",
        "        val_labels_le, val_preds, average=\"weighted\"\n",
        "    ),\n",
        "\n",
        "    \"recall\" : recall_score(\n",
        "        val_labels_le, val_preds, average=\"weighted\"\n",
        "    )\n",
        "}\n",
        "\n",
        "    models_metrics.append(model_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hM0pMjCbj23X"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "model_0 = Pipeline([\n",
        "    (\"tf-idf\", TfidfVectorizer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "model_0.fit(train_sentences, train_labels_le)\n",
        "model_0_val_preds = model_0.predict(val_sentences)\n",
        "\n",
        "add_model_evaluation_to_list(\"model_0\", model_0_val_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHg_RTunlp7r"
      },
      "outputs": [],
      "source": [
        "models_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sEey-WuxXaB"
      },
      "source": [
        "We need to make out text vectorized and then into embeddings to feed to out CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Dataset"
      ],
      "metadata": {
        "id": "MY7yC46Fm7d1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmi4ih7Uxq7H"
      },
      "source": [
        "Finding the average length of words in our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNuPfJjx4j2O"
      },
      "outputs": [],
      "source": [
        "# Find the average number of tokens (words) in training tweets\n",
        "avg_length = round(sum([len(i.split()) for i in train_sentences]) / len(train_sentences))\n",
        "avg_length"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How long of a sentence covers 95% of samples\n",
        "import numpy as np\n",
        "\n",
        "sentence_lens = [len(i.split()) for i in train_sentences]\n",
        "avg_length = int(np.percentile(sentence_lens, 95))\n",
        "avg_length"
      ],
      "metadata": {
        "id": "yaLMN40LrdIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYMKuJCLyRNA"
      },
      "outputs": [],
      "source": [
        "# Creating our Vectorized Text layer and Embedding Layer\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding\n",
        "\n",
        "max_vocab_length = 68000\n",
        "max_length = avg_length\n",
        "\n",
        "text_vectorizer = TextVectorization(\n",
        "    max_tokens = max_vocab_length,\n",
        "    output_sequence_length = max_length,\n",
        "    output_mode = \"int\",\n",
        "    name = \"vectorizing_layer\"\n",
        ")\n",
        "\n",
        "text_vectorizer.adapt(train_sentences) # Adapting text vectorizer\n",
        "text_vocab = text_vectorizer.get_vocabulary()\n",
        "\n",
        "embedding = Embedding(\n",
        "    input_dim = len(text_vocab),\n",
        "    output_dim = 128,\n",
        "    mask_zero=False, # Use masking to handle variable sequence lengths (save space)\n",
        "    name = \"embedding_layer\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Datasets, Making sure our data loads in as fast as possible with tensorflow tf data API"
      ],
      "metadata": {
        "id": "PhKwIYIbgNZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data in tensorflow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_sentences, train_labels_one_hot)\n",
        ")\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_sentences, val_labels_one_hot)\n",
        ")\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_sentences, test_labels_one_hot)\n",
        ")\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "SMmNP8gNgXS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our datasets into Prefetch Datasets for faster loading\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # No need for shuffling as we need our data in sequences\n",
        "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "wP3SMIcTipL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d-7kbsKxHi0"
      },
      "source": [
        "# Creating Model 1 (A deep Sequence Model : A Convolutional Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWXayeit0Hlb"
      },
      "outputs": [],
      "source": [
        "# Creating a saving Directory for TensorBoard\n",
        "SAVE_DIR = \"model_logs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9xu0__90GEj"
      },
      "outputs": [],
      "source": [
        "# Building the Model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.Conv1D(filters=64, kernel_size=5,\n",
        "                  activation=\"relu\", padding=\"same\",\n",
        "                  name=\"convolution_layer\")(x)\n",
        "x = layers.GlobalMaxPool1D(name=\"global_max_pooling_layer\")(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compiling the Model\n",
        "model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the Model\n",
        "history_1 = model_1.fit(train_dataset,\n",
        "                        steps_per_epoch=int(0.1 * len(train_dataset)),\n",
        "                        validation_data=val_dataset,\n",
        "                        validation_steps=int(0.1 * len(val_dataset)), # Only validate on 10% of batches to save time for experimentation\n",
        "                        epochs=5,\n",
        "                        callbacks=[\n",
        "                            create_tensorboard_callback(SAVE_DIR, \"model_1\")\n",
        "                        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rt9z4xo2cfy"
      },
      "outputs": [],
      "source": [
        "model_1.evaluate(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uz0386u2k_v"
      },
      "outputs": [],
      "source": [
        "plot_loss_curves(history_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_preds = tf.argmax(model_1.predict(val_dataset), axis=1)\n",
        "model_1_preds"
      ],
      "metadata": {
        "id": "T6VtsnG1wgND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbRaFSeN2g-u"
      },
      "outputs": [],
      "source": [
        "add_model_evaluation_to_list(\"model_1\", model_1_preds)\n",
        "models_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 (Using a Pretrained Feature Extractor: Pretrained Token Embeddings)"
      ],
      "metadata": {
        "id": "IpzHm9mY0f-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tf_keras\n",
        "\n",
        "def create_tensorboard_callback_tf_keras(dir_name, experiment_name):\n",
        "    \"\"\"\n",
        "        Creates a tensorboard callback to use when training\n",
        "    \"\"\"\n",
        "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y-%m-%d <-> %H-%M-%S\")\n",
        "    file_writer = tf.summary.create_file_writer(log_dir) # Create a FileWriter for TensorBoard\n",
        "    tensorboard_callback = tf_keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "    print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "    return tensorboard_callback"
      ],
      "metadata": {
        "id": "4x2Fu2mW9lyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading Pretrained Feature Extractor"
      ],
      "metadata": {
        "id": "UensrV2VAF40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow_hub as hub\n",
        "\n",
        "# # Loading the Pretrained Feature extractor\n",
        "# feature_extractor = hub.load(\n",
        "#     \"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/4\"\n",
        "# )"
      ],
      "metadata": {
        "id": "L-iZj9kM0jq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tf_keras\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Creating the feature extractoer layer\n",
        "feature_extractor_layer = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "    input_shape=[],\n",
        "    dtype=tf.string,\n",
        "    trainable=False,\n",
        "    name=\"U.S.E\"\n",
        ")"
      ],
      "metadata": {
        "id": "cR4uB0dI1DzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Model\n",
        "model_2 = tf_keras.Sequential([\n",
        "    feature_extractor_layer,\n",
        "    tf_keras.layers.Dense(64),\n",
        "    tf_keras.layers.Dense(num_classes, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compiling the Model\n",
        "model_2.compile(\n",
        "    loss=tf_keras.losses.CategoricalCrossentropy(),\n",
        "    optimizer=tf_keras.optimizers.Adam(),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Fitting the Model\n",
        "history_2 = model_2.fit(\n",
        "    test_dataset,\n",
        "    steps_per_epoch=int(0.1 * len(test_dataset)),\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=int(0.1 * len(val_dataset)),\n",
        "    epochs=5,\n",
        "    callbacks=create_tensorboard_callback_tf_keras(SAVE_DIR, \"model_2\")\n",
        ")"
      ],
      "metadata": {
        "id": "pHzM3l8J1d90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "0Bk43aGN9Tfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_preds = tf.argmax(model_2.predict(val_dataset), axis=1)\n",
        "model_2_preds"
      ],
      "metadata": {
        "id": "xQlaeJrK9eSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_model_evaluation_to_list(\"model_2\", model_2_preds)\n",
        "models_metrics"
      ],
      "metadata": {
        "id": "J2_WWAye-wKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3: Conv 1D with Character Embeddings"
      ],
      "metadata": {
        "id": "2GXqiRhAD0DL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To create a Character level embedding, we need a character level tokenizer"
      ],
      "metadata": {
        "id": "dX7beUDgD5m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a character level tokenizer\n",
        "train_sentences[:5]"
      ],
      "metadata": {
        "id": "R9KIPbHgEw5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make function to split sentences into characters\n",
        "def split_characters(text):\n",
        "    return \" \".join(list(text))\n",
        "\n",
        "# testing the function\n",
        "random_senetence = \"How are you doing\"\n",
        "sentence_split = split_characters(random_senetence)\n",
        "sentence_split"
      ],
      "metadata": {
        "id": "KV7eftnWE9S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split sequence level data splits to character level data splits\n",
        "train_chars = [split_characters(s) for s in train_sentences]\n",
        "val_chars = [split_characters(s) for s in val_sentences]\n",
        "test_chars = [split_characters(s) for s in test_sentences]\n",
        "train_chars[:3]"
      ],
      "metadata": {
        "id": "NVjeUHhCF3JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the average character length\n",
        "char_lens = [len(sentence) for sentence in train_sentences]\n",
        "mean_char_len = np.mean(char_lens)\n",
        "mean_char_len"
      ],
      "metadata": {
        "id": "dPYPsCFqGSok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of our sequences at character level\n",
        "plt.hist(char_lens)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vz0ETZjAJ1wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find what character length covers 95% of sequences\n",
        "output_char_len = int(np.percentile(char_lens, 95))\n",
        "output_char_len"
      ],
      "metadata": {
        "id": "GpKD_AIZKQOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all keyboard characters\n",
        "import string\n",
        "\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "alphabet"
      ],
      "metadata": {
        "id": "ujMcM4JGLhpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Char-Level token vectorizer instance\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2  # add 2 for space and OOV(out of vocab, '[UNK]') token\n",
        "char_vectorizer = TextVectorization(\n",
        "    max_tokens = NUM_CHAR_TOKENS,\n",
        "    output_sequence_length = output_char_len,\n",
        "    output_mode = \"int\",\n",
        "    name = \"char_vectorizer\"\n",
        ")"
      ],
      "metadata": {
        "id": "BOaayN1bME5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt character vectorizer to training characters\n",
        "char_vectorizer.adapt(train_chars)"
      ],
      "metadata": {
        "id": "gh-Qx0ZRPyWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check character vocab stats\n",
        "char_vocab = char_vectorizer.get_vocabulary()\n",
        "print(f\"Length of characters in vocabulary {len(char_vocab)}\")\n",
        "print(f\"Top 5 most common characters {char_vocab[:5]}\")\n",
        "print(f\"Least 5 common words {char_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "ojBMz4lYQIo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "test_rand_char = random.choice(train_chars)\n",
        "print(f\"Character split test: {test_rand_char}\\n\")\n",
        "print(f\"Length of Character split text: {len(test_rand_char.split())}\\n\")\n",
        "vectorized_test_chars = char_vectorizer([test_rand_char])\n",
        "print(f\"Vectorized Character split text: {vectorized_test_chars}\")\n",
        "print(f\"Length of vectorized character split text: {len(vectorized_test_chars[0])}\")"
      ],
      "metadata": {
        "id": "1cc7Em3qSOwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Character level embedding layer"
      ],
      "metadata": {
        "id": "Gg1rcvI9UWsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creatinng the character embedding layer\n",
        "char_embedding = Embedding(\n",
        "    input_dim = len(char_vocab),\n",
        "    output_dim = 25, # this is the size of the char embedding in the paper\n",
        "    mask_zero=False, # Use masking to handle variable sequence lengths (save space)\n",
        "    name = \"character_embedding_layer\"\n",
        ")"
      ],
      "metadata": {
        "id": "NA_3WIR8UbCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out character embedding layer\n",
        "print(f\"Character split text:\\n {test_rand_char}\")\n",
        "character_embed_sample = char_embedding(char_vectorizer([test_rand_char]))\n",
        "print(f\"\\nCharacter embedding:\\n {character_embed_sample}\\n\")\n",
        "print(f\"Character embedding shape: {character_embed_sample.shape}\")"
      ],
      "metadata": {
        "id": "wV1I05yDWxzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Conv1D Model with character embedding"
      ],
      "metadata": {
        "id": "9N9onAziYiP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turning our dataset into tensor slices for fast loading"
      ],
      "metadata": {
        "id": "dQ-DDpkHaOIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data in tensorflow datasets\n",
        "train_dataset_chars = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_chars, train_labels_one_hot)\n",
        ")\n",
        "\n",
        "val_dataset_chars = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_chars, val_labels_one_hot)\n",
        ")\n",
        "\n",
        "test_dataset_chars = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_chars, test_labels_one_hot)\n",
        ")\n",
        "\n",
        "train_dataset_chars"
      ],
      "metadata": {
        "id": "Wnoc4drQaNae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turning our datasets into prefectch datasets for increase in performance and faster training"
      ],
      "metadata": {
        "id": "mJi4_IaMafbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_chars = train_dataset_chars.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset_chars = val_dataset_chars.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset_chars = test_dataset_chars.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "gOH8y-8san-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Building the Model\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = char_vectorizer(inputs)\n",
        "x = char_embedding(x)\n",
        "x = layers.Conv1D(filters=128, kernel_size=5,\n",
        "                  activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.GlobalMaxPool1D(name=\"global_max_pooling_layer\")(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3\")\n",
        "\n",
        "# Compiling the Model\n",
        "model_3.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "# Fit the Model\n",
        "history_3 = model_3.fit(\n",
        "    train_dataset_chars,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=int(0.1 * len(train_dataset_chars)),\n",
        "    validation_data=val_dataset_chars,\n",
        "    validation_steps=int(0.1 * len(val_dataset_chars)),\n",
        "    callbacks=[create_tensorboard_callback(SAVE_DIR, \"model_3\")]\n",
        ")"
      ],
      "metadata": {
        "id": "b_lmEGPzYmWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.evaluate(val_dataset_chars)"
      ],
      "metadata": {
        "id": "OkYm8Fp7bPUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_preds = tf.argmax(model_3.predict(val_dataset_chars), axis=1)\n",
        "model_3_preds"
      ],
      "metadata": {
        "id": "lS2S2dnEgW2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_model_evaluation_to_list(\"model_3\", model_3_preds)\n",
        "models_metrics"
      ],
      "metadata": {
        "id": "hRGyXhDSgcZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4: Combining Pre-trained `token embeddings` with `character embeddings`"
      ],
      "metadata": {
        "id": "OedwaemhSMOl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Create a Token Embedding layer\n",
        "*   Create a Character Embdiing Layer\n",
        "*   Combine Token and Character Embedding layer with a Concatenate Layer\n",
        "*   Build a Series of output layers on top of the concatenated layers\n",
        "*   Construct a Model which takes token and character level sequences as input and produces sequence label probabilities as outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "f_LnGEG0SpaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the feature extractoer layer\n",
        "model_4_feature_extractor_layer = hub.KerasLayer(\n",
        "    \"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\",\n",
        "    trainable=False,\n",
        "    name=\"U.S.E\"\n",
        ")\n",
        "\n",
        "# Set up token embedding layer\n",
        "token_inputs = layers.Input(shape=(), dtype=tf.string, name=\"token_inputs\")\n",
        "token_embeddings = layers.Lambda(lambda x : model_4_feature_extractor_layer(x), output_shape=(512,))(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Model(token_inputs, token_outputs)\n",
        "\n",
        "# Creating a character embedding layer\n",
        "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_inputs\")\n",
        "vectorized_inputs = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embedding(vectorized_inputs)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(24))(char_embeddings)\n",
        "char_model = tf.keras.Model(char_inputs, char_bi_lstm)\n",
        "\n",
        "# Combine with concatenate (token + character), creates a hybrid token embedding\n",
        "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, char_model.output])\n",
        "\n",
        "# Series out output layers, adding in dropout layer\n",
        "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
        "combined_dense = layers.Dense(128, activation=\"relu\")(combined_dropout)\n",
        "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
        "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
        "\n",
        "# Construct Model with token and char inputs\n",
        "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
        "                         outputs=output_layer, name=\"model_4\")"
      ],
      "metadata": {
        "id": "x1WK7FqUSWOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Model summary\n",
        "model_4.summary()"
      ],
      "metadata": {
        "id": "99_zzgW7e7eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Hybrid Model"
      ],
      "metadata": {
        "id": "1soRse-Sfohl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model_4, show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "NfxEgGt_ElJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile Model 4\n",
        "model_4.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "CD2s3KA2Hn7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fast Loading our data using tf.data API\n",
        "train_dataset_text = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars))\n",
        "train_dataset_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
        "train_dataset_hybrid = tf.data.Dataset.zip(train_dataset_text, train_dataset_labels).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset_text = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
        "val_dataset_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_dataset_hybrid = tf.data.Dataset.zip(val_dataset_text, val_dataset_labels).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset_text = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars))\n",
        "test_dataset_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
        "test_dataset_hybrid = tf.data.Dataset.zip(test_dataset_text, test_dataset_labels).batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "WJk2tQYBH1aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the Dataset\n",
        "train_dataset_hybrid, val_dataset_hybrid"
      ],
      "metadata": {
        "id": "o9Ghz0T2JXoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting the Model & Making Predictions"
      ],
      "metadata": {
        "id": "p-Dbx_KZNYKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_4 = model_4.fit(train_dataset_hybrid,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=int(0.1 * len(train_dataset_hybrid)),\n",
        "                        validation_data=val_dataset_hybrid,\n",
        "                        validation_steps=int(0.1 * len(val_dataset_hybrid)),\n",
        "                        callbacks=[create_tensorboard_callback(SAVE_DIR, \"model_4\")])"
      ],
      "metadata": {
        "id": "QhOpAxi5NbEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4.evaluate(val_dataset_hybrid)"
      ],
      "metadata": {
        "id": "687jviW_N7vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_4_preds = tf.argmax(model_4.predict(val_dataset_hybrid), axis=1)\n",
        "model_4_preds"
      ],
      "metadata": {
        "id": "X4biyfU2N_PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_model_evaluation_to_list(\"model_4\", model_4_preds)\n",
        "models_metrics"
      ],
      "metadata": {
        "id": "Sqy0SbPOQCHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🗝️ Note: Any engineered features used to train our model needs to be available at test time. In our case line numbers and total lines are available"
      ],
      "metadata": {
        "id": "bLWo_BHf6yC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Positional Embeddings"
      ],
      "metadata": {
        "id": "1gth1xTHB5mB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Line Numbers"
      ],
      "metadata": {
        "id": "iZlENHxtE59s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many different Line Numbers are there?\n",
        "train_df[\"line_number\"].value_counts()"
      ],
      "metadata": {
        "id": "TJU8rvac6-CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of \"line_number\" column\n",
        "train_df[\"line_number\"].plot(kind=\"hist\")"
      ],
      "metadata": {
        "id": "zjawQWhhCYBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using pure tensorflow to create one hot encoded tensors.\n",
        "# From the distribution it seems like 15 is the cut off for distributions that matter.\n",
        "\n",
        "train_line_numbers_one_hot = tf.one_hot(train_df[\"line_number\"].to_numpy(), depth=15)\n",
        "val_line_numbers_one_hot = tf.one_hot(val_df[\"line_number\"].to_numpy(), depth=15)\n",
        "test_line_numbers_one_hot = tf.one_hot(test_df[\"line_number\"].to_numpy(), depth=15)\n",
        "\n",
        "train_line_numbers_one_hot[:10], train_line_numbers_one_hot.shape"
      ],
      "metadata": {
        "id": "jCoNgNwwCuVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Total Lines"
      ],
      "metadata": {
        "id": "XuHvEbTbE7fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many different number of total lines?\n",
        "train_df[\"total_lines\"].value_counts()"
      ],
      "metadata": {
        "id": "rd-4pV8EE8_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of total lines\n",
        "train_df[\"total_lines\"].plot(kind=\"hist\")"
      ],
      "metadata": {
        "id": "_aD7Z5wRFF5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the coverage of \"total_lines\" value if 20\n",
        "np.percentile(train_df.total_lines, 98)"
      ],
      "metadata": {
        "id": "Y9Dhn3z2FPbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using tensorflow to create one-hot encoded tensors of our \"total_lines\" feature\n",
        "\n",
        "train_total_lines_one_hot = tf.one_hot(train_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "val_total_lines_one_hot = tf.one_hot(val_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df[\"total_lines\"].to_numpy(), depth=20)\n",
        "\n",
        "train_total_lines_one_hot[:10], train_total_lines_one_hot.shape"
      ],
      "metadata": {
        "id": "0MY9XY13GbBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 5: `Pretrained Token Embeddings` + `Character Embeddings` + `Positional  Embeddings`"
      ],
      "metadata": {
        "id": "2Gslix26JEVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Create a Token Level Model\n",
        "2.   Create a Character Level Model\n",
        "3.   Create a model for the \"line_number\" feature\n",
        "4.   Create a model for the \"total_lines\" feature\n",
        "5.   Combine the output for `1` and `2` using `tf.keras.layers.Concatenate( )`\n",
        "6.   Combine the outputs of `3` , `4` , `5`, using `tf.keras.layers.Concatenate( )`\n",
        "\n",
        "7.   Create an output layer to accept the tribrid embedding and output label probabilities\n",
        "8.   Combine the inputs of `1`, `2`, `3`, `4` into a `tf.keras.Model` model"
      ],
      "metadata": {
        "id": "DP87sNUPKgTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Custom Layer for Feature Extraction with Serialization Support\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class FeatureExtractorLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, extractor_layer, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.extractor_layer = extractor_layer  # Store reference\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.extractor_layer(inputs)  # Call the stored layer\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Make the layer serializable\"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"extractor_layer\": tf.keras.utils.serialize_keras_object(self.extractor_layer)\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        \"\"\"Recreate the layer from config\"\"\"\n",
        "        config[\"extractor_layer\"] = tf.keras.utils.deserialize_keras_object(config[\"extractor_layer\"])\n",
        "        return cls(**config)"
      ],
      "metadata": {
        "id": "MbY1yOpdCcja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a Token Model\n",
        "token_inputs = layers.Input(shape=(), dtype=\"string\", name=\"token_inputs\")\n",
        "# token_embeddings = layers.Lambda(lambda x : model_4_feature_extractor_layer(x), output_shape=(512,))(token_inputs)\n",
        "token_embeddings = FeatureExtractorLayer(model_4_feature_extractor_layer)(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
        "token_model = tf.keras.Model(token_inputs, token_outputs)"
      ],
      "metadata": {
        "id": "E32SzT13Lsc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create the Character level Model\n",
        "char_inputs = layers.Input(shape=(1,), dtype=\"string\", name=\"char_inputs\")\n",
        "char_vectorized = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embedding(char_vectorized)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(24))(char_embeddings)\n",
        "char_model = tf.keras.Model(char_inputs, char_bi_lstm)"
      ],
      "metadata": {
        "id": "7wkT4RkXMs-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a Model for the Line Number feature\n",
        "line_number_inputs = layers.Input(shape=(15,),\n",
        "                                  dtype=tf.float32, name=\"line_number_inputs\")\n",
        "x = layers.Dense(32, activation=\"relu\")(line_number_inputs)\n",
        "line_number_model = tf.keras.Model(line_number_inputs, x)"
      ],
      "metadata": {
        "id": "oXS4z6kqNuy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Create a Model for total_lines feature\n",
        "total_lines_inputs = layers.Input(shape=(20,),\n",
        "                                  dtype=tf.float32, name=\"total_lines_inputs\")\n",
        "y = layers.Dense(32, activation=\"relu\")(total_lines_inputs)\n",
        "total_lines_model = tf.keras.Model(total_lines_inputs, y)"
      ],
      "metadata": {
        "id": "s8k5iEO_PQc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine 1 & 2\n",
        "token_char_concat = layers.Concatenate(name=\"token_char_concat\")(\n",
        "    [token_model.output, char_model.output])\n",
        "z = layers.Dropout(0.5)(token_char_concat)\n",
        "z = layers.Dense(256, activation=\"relu\")(z)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "\n",
        "# Combine 3, 4 & 5\n",
        "full_concat = layers.Concatenate(name=\"full_concat\")(\n",
        "    [line_number_model.output, total_lines_model.output, z])"
      ],
      "metadata": {
        "id": "wNBmblodP2-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output layer\n",
        "output_layer = layers.Dense(num_classes, activation=\"softmax\",\n",
        "                            name=\"output_layer\")(full_concat)"
      ],
      "metadata": {
        "id": "KEWUEHg3Uzei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine Inputs\n",
        "model_5 = tf.keras.Model(inputs=[\n",
        "    line_number_model.input,\n",
        "    total_lines_model.input,\n",
        "    token_model.input,\n",
        "    char_model.input\n",
        "], outputs=output_layer, name=\"model_5_tribirid_model\")\n",
        "\n",
        "# Get model summary of Tribrid Model\n",
        "model_5.summary()"
      ],
      "metadata": {
        "id": "e2RGFz-OVRoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plottting Tri-Brid Model"
      ],
      "metadata": {
        "id": "OlxdBCRGXOWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "plot_model(model_5, show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "sTiCIni8XQ36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compiling, Creating Fast loading Pipelines  and Fitting Model"
      ],
      "metadata": {
        "id": "ysY22MGAZPMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **What is label Smoothing?**\n",
        "\n",
        "\n",
        ">For example, if our model gets too confident on a single class, (e.g its prediction probability is really hight), it may get stuck on that class and not consider other classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Really confident: `[0.0, 0.0, 1.0, 0.0]`\n",
        "\n",
        ">What label smoothing does, it assigns some of the value from the highest pred prob to other classes, in turn, hopefully improving generalization:\n",
        "\n",
        " With Label Smoothing: `[0.01, 0.01, 0.96, 0.01]`"
      ],
      "metadata": {
        "id": "GHgLqg8EZyxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the Model\n",
        "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2), # Helps to prevent overfitting\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "HctqIbb7ZS-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Tri-brid datasets using tf.data"
      ],
      "metadata": {
        "id": "NIJIz20Tb7Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For train\n",
        "train_dataset_tribrid_text = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_line_numbers_one_hot, train_total_lines_one_hot, train_sentences, train_chars))\n",
        "train_dataset_tribrid_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
        "train_dataset_tribrid = tf.data.Dataset.zip((train_dataset_tribrid_text,train_dataset_tribrid_labels)\n",
        ").batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# For val\n",
        "val_dataset_tribrid_text = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_line_numbers_one_hot, val_total_lines_one_hot, val_sentences, val_chars))\n",
        "val_dataset_tribrid_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_dataset_tribrid = tf.data.Dataset.zip((val_dataset_tribrid_text,val_dataset_tribrid_labels)\n",
        ").batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# For test\n",
        "test_dataset_tribrid_text = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_line_numbers_one_hot, test_total_lines_one_hot, test_sentences, test_chars))\n",
        "test_dataset_tribrid_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
        "test_dataset_tribrid = tf.data.Dataset.zip((test_dataset_tribrid_text,test_dataset_tribrid_labels)\n",
        ").batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "wPcl3JWvb_0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_tribrid, val_dataset_tribrid_text"
      ],
      "metadata": {
        "id": "9JHJRBSWjVOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the Tribirid Model to the Dataset Pipelines"
      ],
      "metadata": {
        "id": "Isil2RS7jvSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the Model\n",
        "history_5 = model_5.fit(\n",
        "    train_dataset_tribrid,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=int(0.1 * len(train_dataset_tribrid)),\n",
        "    validation_data=val_dataset_tribrid,\n",
        "    validation_steps=int(0.1 * len(val_dataset_tribrid)),\n",
        "    callbacks=[create_tensorboard_callback(SAVE_DIR, \"model_5\")]\n",
        ")"
      ],
      "metadata": {
        "id": "5hTMdNNxj0Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5.evaluate(val_dataset_tribrid)"
      ],
      "metadata": {
        "id": "799FhCpFkOgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5_preds = tf.argmax(model_5.predict(val_dataset_tribrid), axis=1)\n",
        "model_5_preds"
      ],
      "metadata": {
        "id": "yhp5QVGDkR56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_model_evaluation_to_list(\"model_5\", model_5_preds)\n",
        "models_metrics"
      ],
      "metadata": {
        "id": "v2xEIyoOkkQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare Results for all Models"
      ],
      "metadata": {
        "id": "kK9Jnus3pXvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_df = pd.DataFrame(models_metrics)\n",
        "results_df"
      ],
      "metadata": {
        "id": "0mSerFJqpa-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Perfomances"
      ],
      "metadata": {
        "id": "04v3KezHqtnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def plot_model_metric(column):\n",
        "    palette=[\"husl\", \"Set1\", \"Set2\", \"Set3\", \"coolwarm\", \"viridis\"]\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    ax = sns.barplot(data=results_df, x=column, y=\"name\", palette=random.choice(palette), hue=\"name\")\n",
        "    plt.title(f\"Model {column} scores\")\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel(\"Models\")\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fontsize=10)\n",
        "    plt.gca().spines[[\"top\", \"right\"]].set_visible(False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8MdTBHiEqwxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_metric(\"accuracy\")"
      ],
      "metadata": {
        "id": "tZHILxpmsekD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_metric(\"f1\")"
      ],
      "metadata": {
        "id": "IaVK0_IuslKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_metric(\"precision\")"
      ],
      "metadata": {
        "id": "dLKCLm_MsqX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_metric(\"recall\")"
      ],
      "metadata": {
        "id": "eYgqwOyassfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save and Loading Model"
      ],
      "metadata": {
        "id": "x4mvUUwDyE7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_5.save(\"/content/drive/MyDrive/AbstractOptimizer20K.keras\")"
      ],
      "metadata": {
        "id": "bCeMHIscyH-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model(\n",
        "    \"/content/drive/MyDrive/AbstractOptimizer20K.keras\",\n",
        "    custom_objects={\n",
        "        'FeatureExtractorLayer': FeatureExtractorLayer,  # ✅ Add the registered custom layer\n",
        "        'KerasLayer': hub.KerasLayer\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "v6yUchDgynLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions of loaded model\n",
        "loaded_model_preds = tf.argmax(loaded_model.predict(val_dataset_tribrid), axis=1)\n",
        "loaded_model_preds"
      ],
      "metadata": {
        "id": "BzbjuTmdy7-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_5_preds[:10], loaded_model_preds[:10]"
      ],
      "metadata": {
        "id": "GFSH1rUIzFpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv(\"/content/drive/MyDrive/model_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "Pyo5NjLw0RUa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1i76ajv_Ee2Xljkd63_tl_w_UnUGojJfu",
      "authorship_tag": "ABX9TyP6vvuLa5KXQhYTyV1Vfpqf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}